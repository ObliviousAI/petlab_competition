{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# UN PET Lab Competition: Sandbox ðŸœï¸\n",
    "\n",
    "Welcome! This is a short walkthrough to give you a step through the elements of the competition. You can do what you like here and it won't affect your score. Once you experiment and come up with a good strategy, then you can head over to the competition notebooks and copy in any of the cells you think might be helpful.\n",
    "\n",
    "## Prerequisites: Proxy Installation and Quick Start\n",
    "\n",
    "### Step 1: Download installation and start scripts\n",
    "\n",
    "We need these scripts to install proxy for connecting to the Hackathon API server running in Oblvious enclave. Twi scripts oblv-install.sh and oblv-start.sh are downloaded in step 1."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"hOirMHSw2Bv4Nu5W9yvt9O",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "!wget https:\/\/api.oblivious.ai\/oblv-ccli\/scripts\/oblv-install.sh\n",
    "!wget https:\/\/api.oblivious.ai\/oblv-ccli\/scripts\/oblv-start.sh\n",
    "!chmod +x .\/oblv-install.sh\n",
    "!chmod +x .\/oblv-start.sh"
   ],
   "execution_count":97,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"Z857t6RlQQ7JTGaS7YS4wY",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "\n",
    "### Step 2: Install the OBLV on Datalore\n",
    "\n",
    "We've downloaded two scripts to this notebooks file system in the `\/data\/notebook_files\/` folder. The first is `.\/install-oblv.sh` which will help us to download and install the proxy on Linux, specifically Ubuntu:20.04 which this notebook is running in. We will introduce the second script, `.\/start-oblv.sh`, in the next subsection.\n",
    "\n",
    "To run the installation script, first open a terminal. To do this, on the top panel on this window click `Tools >> Terminal`. Your screen will be split and you will have a terminal on the right-hand side. You will already be in the `\/data\/notebook_files\/` so you can go ahead and execute the script:\n",
    "\n",
    "```\n",
    ".\/oblv-install.sh\n",
    "```\n",
    "\n",
    "That should have installed the `oblv` proxy. You can test this by running `oblv --help` and you should see the following output:\n",
    "\n",
    "```\n",
    "Configuration file stored at: \"\/home\/user\/.config\/oblv\/oblv_config.yaml\"\n",
    "\n",
    "oblv 0.4.0\n",
    "Oblivious Software Ltd. <oblivious.ai>\n",
    "Oblivious client app for encrypted connection to secure enclave\n",
    "\n",
    "USAGE:\n",
    "    oblv <SUBCOMMAND>\n",
    "\n",
    "FLAGS:\n",
    "    -h, --help       Prints help information\n",
    "    -V, --version    Prints version information\n",
    "\n",
    "SUBCOMMANDS:\n",
    "    connect      Connect to enclave\n",
    "    help         Prints this message or the help of the given subcommand(s)\n",
    "    keygen       Generate public\/private rsa key pair\n",
    "    reconnect    Reconnect to a previously connected enclave\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Step 3: Run the Proxy as a Background Task\n",
    "\n",
    "Now that we've installed proxy, we can connect to the enclave. Now, to make a connection you both authenticate yourself **and** the attestation of the enclave. To do this, the `oblv` cli proxy needs access to your public\/private key pair which you were emailed (or will be, depending on when you are reading this). \n",
    "\n",
    "These keys should have been labelled `oblv_public.der` and `oblv_private.der` and the keys the contents will be unique for you. If you look at the sidebar on the left, and click the paper clip icon, you can upload those keys as files to the `\/data\/notebook_files\/`. In principle, it doesn't matter what you call these files, but we've hardcoded `oblv_public.der` and `oblv_private.der` into the script `.\/start-oblv.sh` for your convenience. \n",
    "\n",
    "**Note**: The `.\/start-oblv.sh` script is only a template for you to fill in based on the details you received in an email from the organisers. You will have to copy in the PCR codes (there are 3 of these) and the URL hosting the enclave. The script is a single command and there is no reason at all this needs to be a script and not run directly - however, in testing we found some people had issues pasting into the Datalore terminal in the web console, so we put it into a script to make life easier for you. \n",
    "\n",
    "Once the keys are in place, go ahead and hit `.\/start-oblv.sh` in the terminal. This will run the proxy to listen to port 3031 and forward it securely end-to-end into the enclave. Encryption is performed on the fly, so you can just send requests to `localhost:3031` an interact with the enclave as if it is running in locally. "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"lbG5nSew9TTHXHrEMNc1Lq",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"k0vfZq4jc5cJj1GWIHoYFH",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## The Remote-Execution Client\n",
    "\n",
    "### Installation with Pip\n",
    "\n",
    "Now that we've the proxy connection sorted, we can send traffic to the localhost:3031 and it will get directed to the encalve with secure end-to-end encryption and attestation. But we actually want to use the DP libraries locally, and then serialize them into JSON to be sent as REST API calls. Having to write this yourself would just be clunky... so instead we provided a small client library which allows you to pass in the classes of the DP libraries and the serialization\/deserialization and request handling will be taken care for you. Let's go ahead and install that in the next cell: "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"1UUtYoBttgGKmNU2Xxv9rS",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# to be replaced with normal pip install when ready\n",
    "!pip install dp-serial"
   ],
   "execution_count":22,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"G9TBOJnajGDi8Cch73ohJ0",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Running the Client\n",
    "\n",
    "Once the library has been installed, we can create a Client object to send and receive our requests with. This is agnostic of the enclaves (like it would work if you were sending the serialized models to a normal server too). So we simply parameterize the client with the REST API endpoint, so it knows what to speak to. In our case, this is `localhost:3031`:"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"tRsEjLAi9KwqMkJLOFdk66",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#import required libs\n",
    "from dp_serial.client.client import Client\n",
    "import numpy\n",
    "\n",
    "# Ensure the URL is as you've set it in the .\/start-oblv.sh script\n",
    "SANDBOX_URL = \"http:\/\/localhost:3031\" \n",
    "\n",
    "# Ensure the URL is as you've set it in the .\/start-oblv.sh script\n",
    "client_sandbox = Client(SANDBOX_URL)"
   ],
   "execution_count":36,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"uKXpz88hzjKxkj7rykXpyJ",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### The dataset for this Sandbox\n",
    "\n",
    "For this sandbox, we are hosting enclaves with the UCI Car Evaluation Dataset as an example. It has columns named `col1` through `col6` and a columns named `label`. The first three cols range [0-3] and and remaining in [0-2], we've made the label binary. This has nothing to do with the real dataset but is intended as a sandbox to learn from. \n",
    "\n",
    "### Using the client with SmartNoise SQL\n",
    "\n",
    "This is probably the simplest interaction you can do with the client. Just call the below function with the SQL query you wish to execute and the epsilon and delta budget per step of execution. \n",
    "\n",
    "There are 2 steps to executing an SQL query which you should always do. SQL queries may take multiple steps and you may spend more privacy budget than you intended to. So the first step is free and simply returns the epsilon and delta that _would be_ applied if you decide to execute it for real. The second then executes the call for you. \n",
    "\n",
    "The table name you are selecting from is always `comp.comp` and the column names will be as they are documented in the competition notebook. We'll just use the names `col1` and `col2` in the sandbox for demonstration purposes."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"z8WqnMbSGPjzOADJKBAR86",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#Getting SQL query epsilon, delta estimate from API Server\n",
    "estimate = client_sandbox.sql_privacy_estimate(\"SELECT col1, COUNT(labels) FROM comp.comp GROUP BY col1\", 1,0.0001)\n",
    "\n",
    "print(estimate)"
   ],
   "execution_count":29,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"Qe8EomAwfpNbuW2Sg3QrKt",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Now that you know what the [epsilon, delta] cost would be, you can choose to get the result of the query:"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"lGMui96ytQKUPM6wE57P3J",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#Running SQL query using Smartnoise SQL\n",
    "query_result = client_sandbox.sql(\"SELECT col1, COUNT(labels) FROM comp.comp GROUP BY col1\", 1,0.0001)\n",
    "\n",
    "print(query_result)"
   ],
   "execution_count":30,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"l8DY9Q0tJpdnrPSqIk2yKM",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Hurray! We just learned something about the sensitive data via an SQL command (assuming col1 was public and col2 was sensitive for example).\n",
    "\n",
    "Seems simple enough right? The art of the competition, if you were to use the DP-SQL interface, is working out what questions to ask and how much budget you should spend on them - but mastery of that is left up to you.\n",
    "\n",
    "### Using the client with SmartNoise Synth\n",
    "\n",
    "The next, similarly straight forward, interface is that of SmartNoise Synth for synthetic data. To use this, you need to specify what model you would like to use and the privacy budget you would like to apply. There are some advanced additional data you can add, but we'll get to that shortly."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"nZK4X4VIlXglJZEUNwrEsU",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#Generating Data with MWEM Synthesizer \n",
    "mwem_synthetic_data = client_sandbox.synth(\"MWEM\", 0.1, 0.00001)\n",
    "\n",
    "print(mwem_synthetic_data)"
   ],
   "execution_count":31,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"i0RLuuRLbqfAJ237Sw3tr7",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Of course, you do not need to use `MWEM` as your preferred synthetic data. You can choose from `[MWEM, DPCTGAN, MST, PATECTGAN]` and then the second parameter is the epsilon you would like to spend and the second is the delta, as was the case for the SQL queries.\n",
    "\n",
    "You can also request to only use a subset of the columns so that you are not spreading your privacy budget over columns which are not important in your eyes (DP synthetic data in very high dimensions would typically require a lot of epsilon to be accurate). To do this, simply pass in the column names you wish to synthasize like this:"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"bJl3QWrv605rX9m79X6OAd",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#Generating Data with MWEM Synthesizer \n",
    "mwem_synthetic_data = client_sandbox.synth(\"MWEM\", 0.1, 0.00001, select_cols=[\"col1\", \"labels\"], mul_matrix=numpy.array([[1,2]]))\n",
    "\n",
    "print(mwem_synthetic_data)\n",
    "\n",
    "# now use your newly generated synthetic data anyway you like!"
   ],
   "execution_count":37,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"CtLsdvR5cktWdVOnKYd3Nj",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "**âš ï¸ Warning:** Some synthetic methods a slow, but it totally depends on how many columns you select, the synthetic data model used, etc. If for any reason there is a timeout, your score should be uneffected as all queries are performed with exception handling.\n",
    "\n",
    "### Using the client with DiffPrivLib\n",
    "\n",
    "For DiffPrivLib, on the author's advise, we've restricted functionality to executing pipelines which may hold one or many models to be applied sequentially. These follow the DiffPrvLib docs exactly, but before applying your pipeline on the data, you pass it to the client and it will be remotely executed for you and will return the trained model for you. Let's see this in action:"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Jsy2lRxml32lkJT3KxSkov",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.pipeline import Pipeline\n",
    "from diffprivlib import models\n",
    "\n",
    "#Diffprivlib LR Pipeline \n",
    "lr_pipe = Pipeline([\n",
    "    ('lr', models.LogisticRegression(data_norm=5))\n",
    "])\n",
    "\n",
    "# train the model and get the resulting trained model\n",
    "trained_model = client_sandbox.diffprivlib(lr_pipe)"
   ],
   "execution_count":38,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"1Rds0yog8DbEv3y6smc6xU",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Equally, we can do a more complicated pipeline involving scaling, pca and training a logistic regression model:"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"oJJbjoDGu1Zr7cr6bDvDit",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#Preparing Diffprivlib SPLR Pipeline\n",
    "splr_pipe = Pipeline([\n",
    "    ('scaler', models.StandardScaler(bounds=([0, 0, 0, 0, 0, 0], [3, 3, 3, 2, 2, 2]))), # you might not need a scalar here, it's just an example\n",
    "    ('pca', models.PCA(2, data_norm=5, centered=True)),\n",
    "    ('lr', models.LogisticRegression(data_norm=5))\n",
    "])\n",
    "\n",
    "# train the model and get the resulting trained model\n",
    "trained_model = client_sandbox.diffprivlib(splr_pipe)"
   ],
   "execution_count":39,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"ttNWuyLoL6UqJB4I5LQIC9",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "All of accepted DiffPrivLib pipelines are accepted so check out the docs directly. Any model submitted not from DiffPrivLib will be rejected.\n",
    "\n",
    "### Using the client with OpenDP\n",
    "\n",
    "The last framework available is OpenDP. This is in one respect the most flexible, but from another respect the most complicated for someone who is starting out for the first time. \n",
    "\n",
    "In OpenDP, we create pipelines of transformations (like clipping values, selecting columns, etc) and measurements (calculations on the transformed dataset). We typically use the right shift operator, `>>`, to concatenate these transformations and measurements together from end-to-end. \n",
    "\n",
    "The measurement is the part of the pipeline which applies differential privacy. So if there is no measurement as part of the pipeline, then the remote execution will be rejected. Also, same as the other methods, if the epsilon or delta far exceeds the capped epsilon\/delta permitted per query, it will also fail.\n",
    "\n",
    "**ðŸ›‘ Important:** When using OpenDP you must import transformers and measurements as `import dp_serial.opendp_logger.trans as trans` and `import dp_serial.opendp_logger.meas as meas` respectively. OpenDP does not natively keep a track of the abstract syntax tree (AST) of the pipeline and hence it would not be parsable. This `opendp.logger` within `dp_serial` wraps every method in OpenDP such that it can store and export the AST required for remote execution."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"hxycVQUu72z29FTNrHg1Ev",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import dp_serial.opendp_logger.trans as trans\n",
    "import dp_serial.opendp_logger.meas as meas\n",
    "import dp_serial.opendp_logger.comb as comb\n",
    "\n",
    "pipeline = comb.make_pureDP_to_fixed_approxDP(\n",
    "    trans.make_split_dataframe(separator=\",\", col_names=[\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"labels\"]) >>\n",
    "    trans.make_select_column(key=\"labels\", TOA=str) >>\n",
    "    trans.make_cast(TIA=str, TOA=int) >>\n",
    "    trans.make_impute_constant(0) >> \n",
    "    trans.make_clamp(bounds=(0, 1)) >>\n",
    "    trans.make_bounded_sum((0, 1)) >>\n",
    "    meas.make_base_discrete_laplace(scale=1.)\n",
    ")\n",
    "\n",
    "eps, delta = pipeline.map(1)\n",
    "\n",
    "\n",
    "print(eps, delta)\n",
    "\n",
    "opendp_result = client_sandbox.opendp(pipeline)\n",
    "print(opendp_result)\n"
   ],
   "execution_count":108,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "{\"version\": \"0.6.1\", \"ast\": {\"func\": \"make_chain_mt\", \"module\": \"comb\", \"type\": \"Measurement\", \"args\": {\"_tuple\": true, \"_items\": [{\"func\": \"make_base_discrete_laplace\", \"module\": \"meas\", \"type\": \"Measurement\", \"args\": {\"_tuple\": true, \"_items\": []}, \"kwargs\": {\"scale\": 1.0}}, {\"func\": \"make_chain_tt\", \"module\": \"comb\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": [{\"func\": \"make_bounded_sum\", \"module\": \"trans\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": [{\"_tuple\": true, \"_items\": [0, 1]}]}, \"kwargs\": {}}, {\"func\": \"make_chain_tt\", \"module\": \"comb\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": [{\"func\": \"make_clamp\", \"module\": \"trans\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": []}, \"kwargs\": {\"bounds\": {\"_tuple\": true, \"_items\": [0, 1]}}}, {\"func\": \"make_chain_tt\", \"module\": \"comb\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": [{\"func\": \"make_impute_constant\", \"module\": \"trans\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": [0]}, \"kwargs\": {}}, {\"func\": \"make_chain_tt\", \"module\": \"comb\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": [{\"func\": \"make_cast\", \"module\": \"trans\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": []}, \"kwargs\": {\"TIA\": \"py_type:str\", \"TOA\": \"py_type:int\"}}, {\"func\": \"make_chain_tt\", \"module\": \"comb\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": [{\"func\": \"make_select_column\", \"module\": \"trans\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": []}, \"kwargs\": {\"key\": \"labels\", \"TOA\": \"py_type:str\"}}, {\"func\": \"make_split_dataframe\", \"module\": \"trans\", \"type\": \"Transformation\", \"args\": {\"_tuple\": true, \"_items\": []}, \"kwargs\": {\"separator\": \",\", \"col_names\": [\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"labels\"]}}]}, \"kwargs\": {}}]}, \"kwargs\": {}}]}, \"kwargs\": {}}]}, \"kwargs\": {}}]}, \"kwargs\": {}}]}, \"kwargs\": {}}}\n"
     ],
     "output_type":"stream"
    },
    {
     "ename":"TypeError",
     "evalue":"TypeError: cannot unpack non-iterable float object",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 16 in <module>",
      "TypeError: cannot unpack non-iterable float object"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"VDdraUIiNFjc4PGkm879Cj",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Can I only use these outputs?\n",
    "\n",
    "Yes and no. You absolutely do not need to use the above functions and stop there. You have many ways which you can be strategic in your actions before and after. By really understanding `train_x` via visualization, dimensionality reduction, etc you can hopefully make more informed queries, spending less epsilon and delta. Equally, when you get an output, for example, some synthetic data, then you have the opportunity to figure out how to use that output to make informed predictions on `test_x`. "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"71V2lPnKjNLJlsjO49xXIw",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd \n",
    "df = pd.DataFrame({'id':[1,2,3,4,5], 'labels':[0, 0, 0, 0, 1]})\n",
    "res = client_sandbox.submit_predictions_comp(submission=df)\n"
   ],
   "execution_count":106,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "b'{\"status_code\":405,\"error\":\"METHOD_NOT_ALLOWED\"}'\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"UfiRKMumk5bf2vemYvA01F",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}